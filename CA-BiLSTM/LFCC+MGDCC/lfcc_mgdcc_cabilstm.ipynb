{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ecb772d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:18:31.203818Z",
     "iopub.status.busy": "2025-09-05T11:18:31.203531Z",
     "iopub.status.idle": "2025-09-05T11:18:31.207308Z",
     "shell.execute_reply": "2025-09-05T11:18:31.206785Z"
    },
    "papermill": {
     "duration": 0.009616,
     "end_time": "2025-09-05T11:18:31.208311",
     "exception": false,
     "start_time": "2025-09-05T11:18:31.198695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417923b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:18:31.214984Z",
     "iopub.status.busy": "2025-09-05T11:18:31.214784Z",
     "iopub.status.idle": "2025-09-05T11:18:36.090898Z",
     "shell.execute_reply": "2025-09-05T11:18:36.090036Z"
    },
    "papermill": {
     "duration": 4.880999,
     "end_time": "2025-09-05T11:18:36.092372",
     "exception": false,
     "start_time": "2025-09-05T11:18:31.211373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os, pathlib, glob, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "from scipy import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12841adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:18:36.099857Z",
     "iopub.status.busy": "2025-09-05T11:18:36.099422Z",
     "iopub.status.idle": "2025-09-05T11:18:36.173642Z",
     "shell.execute_reply": "2025-09-05T11:18:36.172926Z"
    },
    "papermill": {
     "duration": 0.079141,
     "end_time": "2025-09-05T11:18:36.174882",
     "exception": false,
     "start_time": "2025-09-05T11:18:36.095741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d29b1b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:18:36.181840Z",
     "iopub.status.busy": "2025-09-05T11:18:36.181603Z",
     "iopub.status.idle": "2025-09-05T11:18:36.184925Z",
     "shell.execute_reply": "2025-09-05T11:18:36.184388Z"
    },
    "papermill": {
     "duration": 0.007882,
     "end_time": "2025-09-05T11:18:36.185920",
     "exception": false,
     "start_time": "2025-09-05T11:18:36.178038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "output_nodes = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba184b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:18:36.192566Z",
     "iopub.status.busy": "2025-09-05T11:18:36.192338Z",
     "iopub.status.idle": "2025-09-05T11:22:45.752239Z",
     "shell.execute_reply": "2025-09-05T11:22:45.751318Z"
    },
    "papermill": {
     "duration": 249.567412,
     "end_time": "2025-09-05T11:22:45.756200",
     "exception": false,
     "start_time": "2025-09-05T11:18:36.188788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 111633\n",
      "Validation samples: 37152\n",
      "Testing samples: 37971\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# ======================\n",
    "# Paths for both datasets\n",
    "# ======================\n",
    "train_mgdcc = r\"/kaggle/input/mgdcc-feature/MGDCC/train\"\n",
    "val_mgdcc   = r\"/kaggle/input/mgdcc-feature/MGDCC/dev\"\n",
    "test_mgdcc  = r\"/kaggle/input/mgdcc-feature/MGDCC/test\"\n",
    "\n",
    "train_lfcc = r\"/kaggle/input/lfcc-feature/LFCC_Features/train\"\n",
    "val_lfcc   = r\"/kaggle/input/lfcc-feature/LFCC_Features/dev\"\n",
    "test_lfcc  = r\"/kaggle/input/lfcc-feature/LFCC_Features/test\"\n",
    "\n",
    "# ======================\n",
    "# Dataset Class\n",
    "# ======================\n",
    "class MixedFeatureDataset(Dataset):\n",
    "    def __init__(self, mgdcc_dir, lfcc_dir, max_len=10):\n",
    "        \"\"\"Load MGDCC + LFCC features for the same audio files.\"\"\"\n",
    "        self.files = []\n",
    "        self.class_to_idx = {}\n",
    "        self.max_len = max_len\n",
    "\n",
    "        classes = sorted(entry.name for entry in os.scandir(mgdcc_dir) if entry.is_dir())\n",
    "\n",
    "        for c in classes:\n",
    "            if c not in self.class_to_idx:\n",
    "                self.class_to_idx[c] = len(self.class_to_idx)\n",
    "\n",
    "            mgdcc_cdir = os.path.join(mgdcc_dir, c)\n",
    "            lfcc_cdir = os.path.join(lfcc_dir, c)\n",
    "\n",
    "            for f in os.listdir(mgdcc_cdir):\n",
    "                mgdcc_file = os.path.join(mgdcc_cdir, f)\n",
    "                lfcc_file = os.path.join(lfcc_cdir, f)\n",
    "\n",
    "                if os.path.exists(lfcc_file):   # only keep if both exist\n",
    "                    self.files.append((mgdcc_file, lfcc_file, self.class_to_idx[c]))\n",
    "\n",
    "        random.shuffle(self.files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mgdcc_file, lfcc_file, label = self.files[idx]\n",
    "        try:\n",
    "            mgdcc_vals = scipy.io.loadmat(mgdcc_file)['final'].T\n",
    "            lfcc_vals  = scipy.io.loadmat(lfcc_file)['final'].T\n",
    "    \n",
    "            # Align feature dimension (rows)\n",
    "            max_rows = max(mgdcc_vals.shape[0], lfcc_vals.shape[0])\n",
    "    \n",
    "            def pad_rows(x, target_rows):\n",
    "                if x.shape[0] < target_rows:\n",
    "                    pad_amt = target_rows - x.shape[0]\n",
    "                    return np.pad(x, pad_width=((0, pad_amt), (0, 0)), mode='constant')\n",
    "                return x\n",
    "    \n",
    "            mgdcc_vals = pad_rows(mgdcc_vals, max_rows)\n",
    "            lfcc_vals  = pad_rows(lfcc_vals, max_rows)\n",
    "    \n",
    "            # Now safe to hstack (same rows, concatenate columns)\n",
    "            data = np.hstack([mgdcc_vals, lfcc_vals])\n",
    "    \n",
    "            # Optional: pad/truncate along time dimension (rows = frames)\n",
    "            max_len = 10\n",
    "            if max_len > data.shape[0]:\n",
    "                pad_width = max_len - data.shape[0]\n",
    "                data = np.pad(data, pad_width=((0, pad_width), (0, 0)), mode='constant')\n",
    "            else:\n",
    "                data = data[:max_len, :]\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading files {mgdcc_file}, {lfcc_file}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "        return data, label\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ======================\n",
    "# DataLoader wrapper\n",
    "# ======================\n",
    "class PtDataLoader(DataLoader):\n",
    "    def __init__(self, mgdcc_dir, lfcc_dir, batch_size, shuffle=True):\n",
    "        dataset = MixedFeatureDataset(mgdcc_dir, lfcc_dir)\n",
    "        super().__init__(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# ======================\n",
    "# Load datasets\n",
    "# ======================\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = MixedFeatureDataset(train_mgdcc, train_lfcc)\n",
    "val_dataset   = MixedFeatureDataset(val_mgdcc, val_lfcc)\n",
    "test_dataset  = MixedFeatureDataset(test_mgdcc, test_lfcc)\n",
    "\n",
    "train_dataloader = PtDataLoader(train_mgdcc, train_lfcc, batch_size=batch_size)\n",
    "val_dataloader   = PtDataLoader(val_mgdcc, val_lfcc, batch_size=batch_size)\n",
    "test_dataloader  = PtDataLoader(test_mgdcc, test_lfcc, batch_size=batch_size)\n",
    "\n",
    "train_count = len(train_dataset)\n",
    "val_count   = len(val_dataset)\n",
    "test_count  = len(test_dataset)\n",
    "\n",
    "print(f\"Training samples: {train_count}\\nValidation samples: {val_count}\\nTesting samples: {test_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27ddce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:22:45.763354Z",
     "iopub.status.busy": "2025-09-05T11:22:45.763117Z",
     "iopub.status.idle": "2025-09-05T11:22:45.819812Z",
     "shell.execute_reply": "2025-09-05T11:22:45.819027Z"
    },
    "papermill": {
     "duration": 0.061835,
     "end_time": "2025-09-05T11:22:45.821138",
     "exception": false,
     "start_time": "2025-09-05T11:22:45.759303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_amount = 0.255\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils import data\n",
    "\n",
    "class Res2NetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale=4, kernel_size=3, stride=1, padding=1):\n",
    "        super(Res2NetBlock, self).__init__()\n",
    "        assert out_channels % scale == 0, \"Output channels must be divisible by scale\"\n",
    "        self.scale = scale\n",
    "        self.split_channels = out_channels // scale\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(self.split_channels, self.split_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "            for _ in range(scale - 1)\n",
    "        ])\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv3 = nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        splits = torch.chunk(out, self.scale, dim=1)\n",
    "        out = splits[0]\n",
    "        result = [out]  # Collect outputs here to avoid in-place addition\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            result.append(conv(splits[i + 1]))\n",
    "        out = torch.cat(result, dim=1)  # Concatenate all parts\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn2(out)\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class AMSoftmaxLoss(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.4):\n",
    "        super(AMSoftmaxLoss, self).__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.DoubleTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n",
    "        phi = cosine - self.m\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "        logits = one_hot * phi + (1.0 - one_hot) * cosine\n",
    "        logits *= self.s\n",
    "        return logits\n",
    "\n",
    "\n",
    "class AASIST2(nn.Module):\n",
    "    def __init__(self, d_args):\n",
    "        super(AASIST2, self).__init__()\n",
    "        self.res2net_block1 = Res2NetBlock(d_args['in_channels'], d_args['filts'][0], scale=4)\n",
    "        self.res2net_block2 = Res2NetBlock(d_args['filts'][0], d_args['filts'][1], scale=4)\n",
    "        self.res2net_block3 = Res2NetBlock(d_args['filts'][1], d_args['filts'][2], scale=4)\n",
    "\n",
    "        self.gru = nn.GRU(input_size=d_args['filts'][2],\n",
    "                          hidden_size=d_args['gru_node'],\n",
    "                          num_layers=d_args['nb_gru_layer'],\n",
    "                          batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(d_args['gru_node'], d_args['nb_fc_node'])  # Input size must match GRU hidden size\n",
    "        self.fc2 = nn.Linear(d_args['nb_fc_node'], d_args['nb_classes'])  # Matches number of classes\n",
    "\n",
    "        self.am_softmax = AMSoftmaxLoss(d_args['nb_fc_node'], d_args['nb_classes'])  # Use fc1 output size\n",
    "\n",
    "    def forward(self, x, labels=None, is_test=False):\n",
    "        x = self.res2net_block1(x)\n",
    "        x = self.res2net_block2(x)\n",
    "        x = self.res2net_block3(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # Permute for GRU input [batch, sequence, feature]\n",
    "        self.gru.flatten_parameters()\n",
    "        x, _ = self.gru(x)\n",
    "        x = x[:, -1, :]  # Take last time step output\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        if labels is not None:  # Compute AMSoftmax only if labels are provided\n",
    "            return self.am_softmax(x, labels)\n",
    "        else:  # Return logits for testing/inference\n",
    "            return F.softmax(logits, dim=1)\n",
    "\n",
    "d_args = {\n",
    "    'in_channels': 1,\n",
    "    'filts': [64, 128, 256],\n",
    "    'gru_node': 128,\n",
    "    'nb_gru_layer': 2,\n",
    "    'nb_fc_node': 64,\n",
    "    'nb_classes': 2\n",
    "}\n",
    "model = AASIST2(d_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f0d67b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:22:45.829152Z",
     "iopub.status.busy": "2025-09-05T11:22:45.828775Z",
     "iopub.status.idle": "2025-09-05T11:22:45.832285Z",
     "shell.execute_reply": "2025-09-05T11:22:45.831610Z"
    },
    "papermill": {
     "duration": 0.008126,
     "end_time": "2025-09-05T11:22:45.833428",
     "exception": false,
     "start_time": "2025-09-05T11:22:45.825302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb10d1de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:22:45.840363Z",
     "iopub.status.busy": "2025-09-05T11:22:45.839966Z",
     "iopub.status.idle": "2025-09-05T11:22:45.847927Z",
     "shell.execute_reply": "2025-09-05T11:22:45.847392Z"
    },
    "papermill": {
     "duration": 0.012568,
     "end_time": "2025-09-05T11:22:45.848978",
     "exception": false,
     "start_time": "2025-09-05T11:22:45.836410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop_amount = 0.255\n",
    "\n",
    "# class BiLSTMClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "#         super(BiLSTMClassifier, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.dropout = nn.Dropout(p=drop_amount)\n",
    "#         self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device=x.device, dtype=torch.double)\n",
    "#         c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device=x.device, dtype=torch.double)\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.dropout(out)\n",
    "#         # Extract the output of the last time step from both directions\n",
    "#         last_hidden_state = torch.cat((out[:, -1, :self.hidden_size], out[:, 0, self.hidden_size:]), dim=1)\n",
    "#         output = self.fc(last_hidden_state)\n",
    "#         return output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "drop_amount = 0.255  # keep your global drop\n",
    "\n",
    "class BiLSTMWithCrossAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # BiLSTM (same as yours)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Cross-attention: Q from BiLSTM (dim = 2*hidden),\n",
    "        # K,V from input projected to the same dim.\n",
    "        embed_dim = hidden_size * 2\n",
    "        self.k_proj = nn.Linear(input_size, embed_dim)\n",
    "        self.v_proj = nn.Linear(input_size, embed_dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True,\n",
    "            dropout=drop_amount\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=drop_amount)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, input_size]\n",
    "        # init states in the same device/dtype as x\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size,\n",
    "                         device=x.device, dtype=x.dtype)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size,\n",
    "                         device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))            # [B, T, 2H]\n",
    "\n",
    "        # Project input to K,V for cross-attention\n",
    "        K = self.k_proj(x)                               # [B, T, 2H]\n",
    "        V = self.v_proj(x)                               # [B, T, 2H]\n",
    "\n",
    "        # Cross-attention: queries = BiLSTM outputs\n",
    "        attn_out, _ = self.cross_attn(query=lstm_out, key=K, value=V)  # [B, T, 2H]\n",
    "\n",
    "        # Residual + dropout\n",
    "        fused = self.dropout(lstm_out + attn_out)        # [B, T, 2H]\n",
    "\n",
    "        # Keep your original \"last forward + first backward\" trick\n",
    "        last_hidden_state = torch.cat(\n",
    "            (fused[:, -1, :self.hidden_size], fused[:, 0, self.hidden_size:]),\n",
    "            dim=1\n",
    "        )  # [B, 2H]\n",
    "\n",
    "        output = self.fc(last_hidden_state)              # [B, num_classes]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2d8e7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:22:45.855685Z",
     "iopub.status.busy": "2025-09-05T11:22:45.855435Z",
     "iopub.status.idle": "2025-09-05T11:22:46.182118Z",
     "shell.execute_reply": "2025-09-05T11:22:46.181356Z"
    },
    "papermill": {
     "duration": 0.331369,
     "end_time": "2025-09-05T11:22:46.183463",
     "exception": false,
     "start_time": "2025-09-05T11:22:45.852094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMWithCrossAttention(\n",
       "  (lstm): LSTM(40, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (k_proj): Linear(in_features=40, out_features=512, bias=True)\n",
       "  (v_proj): Linear(in_features=40, out_features=512, bias=True)\n",
       "  (cross_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.255, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parameters\n",
    "input_size = 40\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "# model = BiLSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "model = BiLSTMWithCrossAttention(\n",
    "    input_size=40,   # your MFCC dim\n",
    "    hidden_size=256,\n",
    "    num_layers=2,\n",
    "    num_classes=2,\n",
    "    num_heads=4\n",
    ")\n",
    "model.to(device, dtype=torch.double)  # stays compatible with your training loop\n",
    "\n",
    "model.to(device, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac97f35f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:22:46.191251Z",
     "iopub.status.busy": "2025-09-05T11:22:46.190741Z",
     "iopub.status.idle": "2025-09-05T11:22:46.194006Z",
     "shell.execute_reply": "2025-09-05T11:22:46.193430Z"
    },
    "papermill": {
     "duration": 0.008083,
     "end_time": "2025-09-05T11:22:46.194969",
     "exception": false,
     "start_time": "2025-09-05T11:22:46.186886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f378434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:22:46.201702Z",
     "iopub.status.busy": "2025-09-05T11:22:46.201452Z",
     "iopub.status.idle": "2025-09-05T11:22:50.761687Z",
     "shell.execute_reply": "2025-09-05T11:22:50.761049Z"
    },
    "papermill": {
     "duration": 4.565123,
     "end_time": "2025-09-05T11:22:50.763131",
     "exception": false,
     "start_time": "2025-09-05T11:22:46.198008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff5faf8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:22:50.771320Z",
     "iopub.status.busy": "2025-09-05T11:22:50.770755Z",
     "iopub.status.idle": "2025-09-05T11:22:50.774883Z",
     "shell.execute_reply": "2025-09-05T11:22:50.774234Z"
    },
    "papermill": {
     "duration": 0.009135,
     "end_time": "2025-09-05T11:22:50.775911",
     "exception": false,
     "start_time": "2025-09-05T11:22:50.766776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMWithCrossAttention(\n",
      "  (lstm): LSTM(40, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (k_proj): Linear(in_features=40, out_features=512, bias=True)\n",
      "  (v_proj): Linear(in_features=40, out_features=512, bias=True)\n",
      "  (cross_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.255, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2c2c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T11:22:50.782856Z",
     "iopub.status.busy": "2025-09-05T11:22:50.782672Z",
     "iopub.status.idle": "2025-09-05T14:55:40.158559Z",
     "shell.execute_reply": "2025-09-05T14:55:40.157684Z"
    },
    "papermill": {
     "duration": 12769.381307,
     "end_time": "2025-09-05T14:55:40.160239",
     "exception": false,
     "start_time": "2025-09-05T11:22:50.778932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 [Train]: 100%|██████████| 3489/3489 [16:40<00:00,  3.49it/s, loss=0.204]\n",
      "Epoch 1/40 [Val]: 100%|██████████| 1161/1161 [06:17<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40, Train Loss: 0.2500, Train Accuracy: 0.8961, Val Accuracy: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 [Train]: 100%|██████████| 3489/3489 [05:23<00:00, 10.78it/s, loss=0.274]\n",
      "Epoch 2/40 [Val]: 100%|██████████| 1161/1161 [00:57<00:00, 20.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/40, Train Loss: 0.1602, Train Accuracy: 0.9378, Val Accuracy: 0.9241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 [Train]: 100%|██████████| 3489/3489 [03:20<00:00, 17.36it/s, loss=0.291]\n",
      "Epoch 3/40 [Val]: 100%|██████████| 1161/1161 [00:59<00:00, 19.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/40, Train Loss: 0.1223, Train Accuracy: 0.9536, Val Accuracy: 0.9114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 [Train]: 100%|██████████| 3489/3489 [03:24<00:00, 17.06it/s, loss=0.168]\n",
      "Epoch 4/40 [Val]: 100%|██████████| 1161/1161 [00:59<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/40, Train Loss: 0.1283, Train Accuracy: 0.9565, Val Accuracy: 0.9178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 [Train]: 100%|██████████| 3489/3489 [03:15<00:00, 17.83it/s, loss=0.0419]\n",
      "Epoch 5/40 [Val]: 100%|██████████| 1161/1161 [00:56<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/40, Train Loss: 0.0906, Train Accuracy: 0.9653, Val Accuracy: 0.9229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 [Train]: 100%|██████████| 3489/3489 [03:24<00:00, 17.07it/s, loss=0.0111]\n",
      "Epoch 6/40 [Val]: 100%|██████████| 1161/1161 [00:56<00:00, 20.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/40, Train Loss: 0.0822, Train Accuracy: 0.9700, Val Accuracy: 0.9335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 [Train]: 100%|██████████| 3489/3489 [03:23<00:00, 17.16it/s, loss=0.0112]\n",
      "Epoch 7/40 [Val]: 100%|██████████| 1161/1161 [00:58<00:00, 19.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/40, Train Loss: 0.0773, Train Accuracy: 0.9720, Val Accuracy: 0.9365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 [Train]: 100%|██████████| 3489/3489 [03:39<00:00, 15.93it/s, loss=0.047]\n",
      "Epoch 8/40 [Val]: 100%|██████████| 1161/1161 [01:18<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/40, Train Loss: 0.1158, Train Accuracy: 0.9667, Val Accuracy: 0.9069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 [Train]: 100%|██████████| 3489/3489 [03:35<00:00, 16.20it/s, loss=0.00185]\n",
      "Epoch 9/40 [Val]: 100%|██████████| 1161/1161 [00:57<00:00, 20.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/40, Train Loss: 0.0632, Train Accuracy: 0.9777, Val Accuracy: 0.9273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 [Train]: 100%|██████████| 3489/3489 [03:23<00:00, 17.10it/s, loss=0.195]\n",
      "Epoch 10/40 [Val]: 100%|██████████| 1161/1161 [00:57<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/40, Train Loss: 0.0823, Train Accuracy: 0.9711, Val Accuracy: 0.9341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 [Train]: 100%|██████████| 3489/3489 [04:08<00:00, 14.05it/s, loss=0.0128]\n",
      "Epoch 11/40 [Val]: 100%|██████████| 1161/1161 [01:33<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/40, Train Loss: 0.0729, Train Accuracy: 0.9742, Val Accuracy: 0.9274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 [Train]: 100%|██████████| 3489/3489 [04:17<00:00, 13.55it/s, loss=0.214]\n",
      "Epoch 12/40 [Val]: 100%|██████████| 1161/1161 [00:56<00:00, 20.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/40, Train Loss: 0.0667, Train Accuracy: 0.9784, Val Accuracy: 0.9198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 [Train]: 100%|██████████| 3489/3489 [03:18<00:00, 17.61it/s, loss=0.00502]\n",
      "Epoch 13/40 [Val]: 100%|██████████| 1161/1161 [00:57<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/40, Train Loss: 0.0646, Train Accuracy: 0.9775, Val Accuracy: 0.9397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 [Train]: 100%|██████████| 3489/3489 [03:10<00:00, 18.35it/s, loss=0.0104]\n",
      "Epoch 14/40 [Val]: 100%|██████████| 1161/1161 [00:55<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/40, Train Loss: 0.0702, Train Accuracy: 0.9797, Val Accuracy: 0.9359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 [Train]: 100%|██████████| 3489/3489 [03:15<00:00, 17.84it/s, loss=0.00499]\n",
      "Epoch 15/40 [Val]: 100%|██████████| 1161/1161 [00:56<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/40, Train Loss: 0.0640, Train Accuracy: 0.9774, Val Accuracy: 0.9380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 [Train]: 100%|██████████| 3489/3489 [04:03<00:00, 14.33it/s, loss=0.0214]\n",
      "Epoch 16/40 [Val]: 100%|██████████| 1161/1161 [01:48<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/40, Train Loss: 0.0693, Train Accuracy: 0.9787, Val Accuracy: 0.9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 [Train]: 100%|██████████| 3489/3489 [04:58<00:00, 11.68it/s, loss=0.105]\n",
      "Epoch 17/40 [Val]: 100%|██████████| 1161/1161 [01:14<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/40, Train Loss: 0.0519, Train Accuracy: 0.9826, Val Accuracy: 0.9223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 [Train]: 100%|██████████| 3489/3489 [03:48<00:00, 15.29it/s, loss=0.00538]\n",
      "Epoch 18/40 [Val]: 100%|██████████| 1161/1161 [01:18<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/40, Train Loss: 0.0636, Train Accuracy: 0.9787, Val Accuracy: 0.9351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 [Train]: 100%|██████████| 3489/3489 [05:06<00:00, 11.37it/s, loss=0.00296]\n",
      "Epoch 19/40 [Val]: 100%|██████████| 1161/1161 [01:18<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/40, Train Loss: 0.0586, Train Accuracy: 0.9799, Val Accuracy: 0.9297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 [Train]: 100%|██████████| 3489/3489 [04:22<00:00, 13.30it/s, loss=0.0107]\n",
      "Epoch 20/40 [Val]: 100%|██████████| 1161/1161 [01:17<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/40, Train Loss: 0.1108, Train Accuracy: 0.9680, Val Accuracy: 0.9213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 [Train]: 100%|██████████| 3489/3489 [04:01<00:00, 14.45it/s, loss=0.3]\n",
      "Epoch 21/40 [Val]: 100%|██████████| 1161/1161 [00:58<00:00, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/40, Train Loss: 0.0713, Train Accuracy: 0.9747, Val Accuracy: 0.9237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 [Train]: 100%|██████████| 3489/3489 [03:35<00:00, 16.18it/s, loss=0.0478]\n",
      "Epoch 22/40 [Val]: 100%|██████████| 1161/1161 [01:03<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/40, Train Loss: 0.0669, Train Accuracy: 0.9761, Val Accuracy: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 [Train]: 100%|██████████| 3489/3489 [03:22<00:00, 17.19it/s, loss=0.249]\n",
      "Epoch 23/40 [Val]: 100%|██████████| 1161/1161 [00:56<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/40, Train Loss: 0.0611, Train Accuracy: 0.9792, Val Accuracy: 0.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 [Train]: 100%|██████████| 3489/3489 [03:14<00:00, 17.90it/s, loss=0.00676]\n",
      "Epoch 24/40 [Val]: 100%|██████████| 1161/1161 [00:54<00:00, 21.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/40, Train Loss: 0.0581, Train Accuracy: 0.9801, Val Accuracy: 0.9355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 [Train]: 100%|██████████| 3489/3489 [03:12<00:00, 18.15it/s, loss=0.0335]\n",
      "Epoch 25/40 [Val]: 100%|██████████| 1161/1161 [00:56<00:00, 20.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/40, Train Loss: 0.0619, Train Accuracy: 0.9783, Val Accuracy: 0.9331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 [Train]: 100%|██████████| 3489/3489 [03:24<00:00, 17.04it/s, loss=0.0224]\n",
      "Epoch 26/40 [Val]: 100%|██████████| 1161/1161 [01:09<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/40, Train Loss: 0.0563, Train Accuracy: 0.9810, Val Accuracy: 0.9374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 [Train]: 100%|██████████| 3489/3489 [03:44<00:00, 15.54it/s, loss=0.0281]\n",
      "Epoch 27/40 [Val]: 100%|██████████| 1161/1161 [01:06<00:00, 17.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/40, Train Loss: 0.0507, Train Accuracy: 0.9838, Val Accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 [Train]: 100%|██████████| 3489/3489 [03:51<00:00, 15.04it/s, loss=0.163]\n",
      "Epoch 28/40 [Val]: 100%|██████████| 1161/1161 [01:08<00:00, 16.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/40, Train Loss: 0.0633, Train Accuracy: 0.9785, Val Accuracy: 0.9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 [Train]: 100%|██████████| 3489/3489 [03:52<00:00, 15.00it/s, loss=0.0465]\n",
      "Epoch 29/40 [Val]: 100%|██████████| 1161/1161 [01:12<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/40, Train Loss: 0.0516, Train Accuracy: 0.9819, Val Accuracy: 0.9296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 [Train]: 100%|██████████| 3489/3489 [05:04<00:00, 11.45it/s, loss=0.0464]\n",
      "Epoch 30/40 [Val]: 100%|██████████| 1161/1161 [02:04<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/40, Train Loss: 0.0930, Train Accuracy: 0.9813, Val Accuracy: 0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 [Train]: 100%|██████████| 3489/3489 [05:36<00:00, 10.35it/s, loss=0.000773]\n",
      "Epoch 31/40 [Val]: 100%|██████████| 1161/1161 [01:12<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/40, Train Loss: 0.0460, Train Accuracy: 0.9840, Val Accuracy: 0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 [Train]: 100%|██████████| 3489/3489 [03:10<00:00, 18.30it/s, loss=0.0169]\n",
      "Epoch 32/40 [Val]: 100%|██████████| 1161/1161 [00:55<00:00, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32/40, Train Loss: 0.0665, Train Accuracy: 0.9810, Val Accuracy: 0.9367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 [Train]: 100%|██████████| 3489/3489 [03:12<00:00, 18.16it/s, loss=0.00259]\n",
      "Epoch 33/40 [Val]: 100%|██████████| 1161/1161 [00:54<00:00, 21.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/40, Train Loss: 0.0366, Train Accuracy: 0.9883, Val Accuracy: 0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 [Train]: 100%|██████████| 3489/3489 [03:15<00:00, 17.89it/s, loss=0.0145]\n",
      "Epoch 34/40 [Val]: 100%|██████████| 1161/1161 [00:55<00:00, 20.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/40, Train Loss: 0.0665, Train Accuracy: 0.9777, Val Accuracy: 0.9261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 [Train]: 100%|██████████| 3489/3489 [03:16<00:00, 17.78it/s, loss=0.124]\n",
      "Epoch 35/40 [Val]: 100%|██████████| 1161/1161 [00:56<00:00, 20.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/40, Train Loss: 0.1174, Train Accuracy: 0.9749, Val Accuracy: 0.9284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 [Train]: 100%|██████████| 3489/3489 [03:49<00:00, 15.18it/s, loss=0.00498]\n",
      "Epoch 36/40 [Val]: 100%|██████████| 1161/1161 [00:59<00:00, 19.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/40, Train Loss: 0.0557, Train Accuracy: 0.9799, Val Accuracy: 0.9371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 [Train]: 100%|██████████| 3489/3489 [03:22<00:00, 17.20it/s, loss=0.171]\n",
      "Epoch 37/40 [Val]: 100%|██████████| 1161/1161 [00:58<00:00, 19.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/40, Train Loss: 0.0551, Train Accuracy: 0.9822, Val Accuracy: 0.9263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 [Train]: 100%|██████████| 3489/3489 [03:17<00:00, 17.64it/s, loss=0.0193]\n",
      "Epoch 38/40 [Val]: 100%|██████████| 1161/1161 [01:05<00:00, 17.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/40, Train Loss: 0.0446, Train Accuracy: 0.9851, Val Accuracy: 0.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 [Train]: 100%|██████████| 3489/3489 [03:41<00:00, 15.77it/s, loss=0.12]\n",
      "Epoch 39/40 [Val]: 100%|██████████| 1161/1161 [01:06<00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/40, Train Loss: 0.0702, Train Accuracy: 0.9837, Val Accuracy: 0.9304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 [Train]: 100%|██████████| 3489/3489 [03:26<00:00, 16.91it/s, loss=0.00367]\n",
      "Epoch 40/40 [Val]: 100%|██████████| 1161/1161 [01:02<00:00, 18.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/40, Train Loss: 0.0530, Train Accuracy: 0.9812, Val Accuracy: 0.9340\n",
      "Max Accuracy: 0.9411606373815676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Model training and testing\n",
    "n_total_steps = len(train_dataloader)\n",
    "train_accuracy_list = []\n",
    "train_loss_list = []\n",
    "val_accuracy_list = []\n",
    "max_acc = 0\n",
    "num_epochs = 40\n",
    "pred_labels = []\n",
    "act_labels = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Training loop with tqdm\n",
    "    train_loop = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for batch_idx, (images, labels) in train_loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # stats\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        train_accuracy += int((prediction == labels).sum().item())\n",
    "\n",
    "        # Update tqdm bar\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_accuracy /= train_count\n",
    "    train_loss /= train_count\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    model.eval()\n",
    "    val_accuracy = 0.0\n",
    "    pred = []\n",
    "    lab = []\n",
    "\n",
    "    val_loop = tqdm(val_dataloader, total=len(val_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loop:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, prediction = torch.max(outputs.data, 1)\n",
    "            val_accuracy += int((prediction == labels).sum().item())\n",
    "            pred.extend(prediction.cpu().tolist())\n",
    "            lab.extend(labels.cpu().tolist())\n",
    "\n",
    "    val_accuracy /= val_count\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "    if max_acc < val_accuracy:\n",
    "        max_acc = val_accuracy\n",
    "        pred_labels = pred\n",
    "        actual_labels = lab\n",
    "        torch.save(model, \"best_accuracy_model_BiLSTM.pth\")\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Max Accuracy:\", max_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace9bef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T14:56:00.327924Z",
     "iopub.status.busy": "2025-09-05T14:56:00.327379Z",
     "iopub.status.idle": "2025-09-05T15:02:36.865514Z",
     "shell.execute_reply": "2025-09-05T15:02:36.864729Z"
    },
    "papermill": {
     "duration": 416.527129,
     "end_time": "2025-09-05T15:02:46.859819",
     "exception": false,
     "start_time": "2025-09-05T14:55:50.332690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 95.27%\n"
     ]
    }
   ],
   "source": [
    "best_model=torch.load(\"best_accuracy_model_BiLSTM.pth\", weights_only=False)\n",
    "best_model.to(device, dtype=torch.double)\n",
    "best_model.eval()\n",
    "\n",
    "# Initialize variables to store results\n",
    "testing_accuracy = 0.0\n",
    "pred_labels = []\n",
    "act_labels = []\n",
    "\n",
    "# Iterate over the test dataloader\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(test_dataloader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "\n",
    "        # Forward pass (only input x)\n",
    "        outputs = best_model(images)\n",
    "\n",
    "        # Get predictions by taking the index with the highest score\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Calculate the number of correct predictions\n",
    "        testing_accuracy += (prediction == labels).sum().item()\n",
    "\n",
    "        # Store predicted and actual labels\n",
    "        pred_labels.extend(prediction.cpu().tolist())\n",
    "        act_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "# Calculate the testing accuracy\n",
    "testing_accuracy /= len(test_dataloader.dataset)\n",
    "\n",
    "# Print the testing accuracy\n",
    "print(f\"Testing Accuracy: {testing_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbe6a8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T15:03:07.071602Z",
     "iopub.status.busy": "2025-09-05T15:03:07.071309Z",
     "iopub.status.idle": "2025-09-05T15:03:07.074797Z",
     "shell.execute_reply": "2025-09-05T15:03:07.074158Z"
    },
    "papermill": {
     "duration": 10.109399,
     "end_time": "2025-09-05T15:03:07.075920",
     "exception": false,
     "start_time": "2025-09-05T15:02:56.966521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Calculate the confusion matrix\n",
    "# import seaborn as sns\n",
    "# conf_mat = confusion_matrix(act_labels, pred_labels)\n",
    "# # Plot confusion matrix heat map\n",
    "# sns.heatmap(conf_mat, cmap=\"flare\",annot=True, fmt = \"g\", \n",
    "#             cbar_kws={\"label\":\"color bar\"},\n",
    "#             xticklabels=train_dataset.classes,\n",
    "#             yticklabels=train_dataset.classes)\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"Actual\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.savefig(\"ConfusionMatrix_BiLSTM.png\")\n",
    "# plt.show()\n",
    "# from sklearn.metrics import f1_score\n",
    "# f1_score = f1_score(pred_labels, act_labels, average='macro')\n",
    "# print('F1 Score : ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dc0a848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T15:03:27.038802Z",
     "iopub.status.busy": "2025-09-05T15:03:27.038222Z",
     "iopub.status.idle": "2025-09-05T15:03:27.059993Z",
     "shell.execute_reply": "2025-09-05T15:03:27.059026Z"
    },
    "papermill": {
     "duration": 9.842593,
     "end_time": "2025-09-05T15:03:27.061194",
     "exception": false,
     "start_time": "2025-09-05T15:03:17.218601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The equal error rate is 0.047\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "\"\"\"\n",
    "Python compute equal error rate (eer)\n",
    "ONLY tested on binary classification\n",
    "\n",
    ":param label: ground-truth label, should be a 1-d list or np.array, each element represents the ground-truth label of one sample\n",
    ":param pred: model prediction, should be a 1-d list or np.array, each element represents the model prediction of one sample\n",
    ":param positive_label: the class that is viewed as positive class when computing EER\n",
    ":return: equal error rate (EER)\n",
    "\"\"\"\n",
    "def compute_eer(label, pred):\n",
    "    # all fpr, tpr, fnr, fnr, threshold are lists (in the format of np.array)\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred)\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # the threshold of fnr == fpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # theoretically eer from fpr and eer from fnr should be identical but they can be slightly differ in reality\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "\n",
    "    # return the mean of eer from fpr and from fnr\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "\n",
    "eer = compute_eer(act_labels, pred_labels)\n",
    "print('The equal error rate is {:.3f}'.format(eer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7a929",
   "metadata": {
    "papermill": {
     "duration": 9.983086,
     "end_time": "2025-09-05T15:03:47.084795",
     "exception": false,
     "start_time": "2025-09-05T15:03:37.101709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10901713,
     "datasetId": 6539291,
     "sourceId": 10567782,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13541698,
     "datasetId": 8157101,
     "sourceId": 12892670,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13531.798014,
   "end_time": "2025-09-05T15:03:58.973731",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-05T11:18:27.175717",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
